{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_ANN_CLASSIFICA_julho_28_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_Julho_2020_using/blob/master/03_ANN_CLASSIFICA_julho_28_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYo6yMKOjiJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the 'transform' module from 'skimage'\n",
        "from skimage import transform "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La6dFeJujmZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJAZpHlAi6Vq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "273d2192-19b0-4511-c60a-26fe8c246e17"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_Julho_2020_using\n",
        "%cd marquesgabi_Julho_2020_using\n",
        "Transfere='FotosTreino882_and_Segm.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_Julho_2020_using' already exists and is not an empty directory.\n",
            "/content/marquesgabi_Julho_2020_using\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYx5ot9RrYYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First step: get the segmented file (photos stored in csv file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Bl70iLs7FF",
        "colab_type": "text"
      },
      "source": [
        "# First step: get the segmented file (photos stored in csv file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C35MvO7VjUA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [] #name files\n",
        "\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVo_UFsfbTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(labels[0])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuDx6iA-w0aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Num=len(labels)\n",
        "df=pd.read_csv(labels[0])\n",
        "for i in range(1,Num):\n",
        "  df_new=pd.read_csv(labels[i])\n",
        "  frames = [df, df_new]\n",
        "  df= pd.concat(frames, ignore_index=True)\n",
        "\n",
        "y_valor=df['Type']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuwLJl-21LQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "77f8c3a5-2c81-4365-82ec-a6296b223a58"
      },
      "source": [
        "print(df.tail())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Unnamed: 0 Type          0  ...        782        783  Width\n",
            "985           7    B   84.33883  ...    7.14600    7.38484  142.0\n",
            "986           8    I  104.73698  ...    2.05457    0.38150  163.0\n",
            "987           9    I   75.70248  ...  107.38844  106.00001  154.0\n",
            "988          10    I   56.64465  ...    3.15316    0.40107  187.0\n",
            "989          11    B  111.50165  ...    0.00000    0.00000  194.0\n",
            "\n",
            "[5 rows x 787 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwELp2Lv1NKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "5a64ef48-bffa-4671-ad4d-889b5c8d0e42"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0 Type        0        1  ...      781      782      783  Width\n",
            "0           0    G  0.00000  0.00000  ...  0.68642  0.68631  0.68769    NaN\n",
            "1           1    G  0.00000  0.00000  ...  0.47277  0.49262  0.50909    NaN\n",
            "2           2    G  0.45321  0.52257  ...  0.54645  0.54919  0.55439    NaN\n",
            "3           3    G  0.75367  0.64590  ...  0.43356  0.48524  0.61876    NaN\n",
            "4           4    G  0.94521  0.89736  ...  0.98942  0.98108  0.95838    NaN\n",
            "\n",
            "[5 rows x 787 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMCpdgC91ON0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "aba9b85f-52af-4d23-89a7-9c6b1d4ee35c"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Unnamed: 0 Type          0  ...        782        783  Width\n",
            "0             0    G    0.00000  ...    0.68631    0.68769    NaN\n",
            "1             1    G    0.00000  ...    0.49262    0.50909    NaN\n",
            "2             2    G    0.45321  ...    0.54919    0.55439    NaN\n",
            "3             3    G    0.75367  ...    0.48524    0.61876    NaN\n",
            "4             4    G    0.94521  ...    0.98108    0.95838    NaN\n",
            "..          ...  ...        ...  ...        ...        ...    ...\n",
            "985           7    B   84.33883  ...    7.14600    7.38484  142.0\n",
            "986           8    I  104.73698  ...    2.05457    0.38150  163.0\n",
            "987           9    I   75.70248  ...  107.38844  106.00001  154.0\n",
            "988          10    I   56.64465  ...    3.15316    0.40107  187.0\n",
            "989          11    B  111.50165  ...    0.00000    0.00000  194.0\n",
            "\n",
            "[990 rows x 787 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wq1CSlXUhjEv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "13eb455c-b76d-46a4-970c-b8e00f16c28c"
      },
      "source": [
        "df_teste=pd.read_csv(labels[2])\n",
        "print(df_teste.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  Width Type          0  ...       780       781        782        783\n",
            "0           0    157    I   71.90012  ...  49.57118  35.95513    7.75954    1.03862\n",
            "1           1    124    I  127.06763  ...  94.57544  98.71695  100.52861  100.84391\n",
            "2           2    182    G   87.10651  ...   5.33136   5.84024    5.89941    5.65089\n",
            "3           3    101    I   99.01176  ...  35.15087  41.39006   43.76875   44.09146\n",
            "4           4    183    I   56.30844  ...  52.96656  53.65711   53.44656   53.11703\n",
            "\n",
            "[5 rows x 787 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkMXproqxZig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "8c491211-976e-476d-f8d8-f61f7d292a95"
      },
      "source": [
        "print(df.tail())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Unnamed: 0 Type          0  ...        782        783  Width\n",
            "985           7    B   84.33883  ...    7.14600    7.38484  142.0\n",
            "986           8    I  104.73698  ...    2.05457    0.38150  163.0\n",
            "987           9    I   75.70248  ...  107.38844  106.00001  154.0\n",
            "988          10    I   56.64465  ...    3.15316    0.40107  187.0\n",
            "989          11    B  111.50165  ...    0.00000    0.00000  194.0\n",
            "\n",
            "[5 rows x 787 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpuIGIU-pkiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fotos=df.drop(['Unnamed: 0','Type','Width'], axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfXvfb-n9LPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fotos=df.drop(['Unnamed: 0','Type','Width'], axis=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2PqY2SbzFI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f227aa2-7710-49f0-d37e-56de1fa009ee"
      },
      "source": [
        "print(np.array(Fotos).shape )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(990, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDoUMG4vsmoa",
        "colab_type": "text"
      },
      "source": [
        "# Second step: create the ann to evaluate which is a grain in photos segmented\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biuEDBlM4H_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images28=np.array(df)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1FNc_PgKZ3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "56fb77c8-daed-4a61-f400-e2f36eb5254a"
      },
      "source": [
        "print(y_valor)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      G\n",
            "1      G\n",
            "2      G\n",
            "3      G\n",
            "4      G\n",
            "      ..\n",
            "985    B\n",
            "986    I\n",
            "987    I\n",
            "988    I\n",
            "989    B\n",
            "Name: Type, Length: 990, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgZ5KNGh8mzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_new=[]\n",
        "Grain=0\n",
        "Others=0\n",
        "Cont=-1\n",
        "Others_Local=[]\n",
        "Grain_Local=[]\n",
        "for x in y_valor:\n",
        "  Cont=Cont+1\n",
        "  if re.search('G', x):\n",
        "    y_new.append(1)\n",
        "    Grain=Grain+1\n",
        "    Grain_Local.append(Cont)\n",
        "  else:\n",
        "    y_new.append(2)\n",
        "    Others=Others+1\n",
        "    Others_Local.append(Cont)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4IZyKSa_Xvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ac516659-1d84-4ebe-995c-1f7e8c4feb2b"
      },
      "source": [
        "print('Graos=',Grain)\n",
        "print('Others=',Others)\n",
        "print('Others local',Others_Local)\n",
        "print('Grain local',Grain_Local)\n",
        "\n",
        "sampled_list = random.sample(Others_Local,Grain)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graos= 254\n",
            "Others= 736\n",
            "Others local [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 911, 912, 913, 914, 915, 916, 917, 918, 919, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 947, 948, 949, 950, 951, 952, 953, 954, 955, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 983, 984, 985, 986, 987, 988, 989]\n",
            "Grain local [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 36, 37, 38, 39, 40, 41, 42, 43, 72, 73, 74, 75, 76, 108, 109, 110, 111, 112, 113, 114, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 206, 207, 208, 209, 210, 211, 212, 213, 242, 243, 244, 245, 246, 247, 248, 249, 268, 269, 270, 271, 272, 273, 274, 275, 276, 304, 305, 306, 307, 308, 309, 310, 340, 341, 342, 343, 344, 345, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 402, 403, 404, 405, 406, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 500, 501, 502, 503, 504, 505, 506, 507, 508, 536, 537, 538, 539, 540, 541, 542, 543, 544, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 588, 589, 590, 591, 592, 593, 594, 624, 625, 626, 627, 628, 629, 630, 631, 632, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 696, 697, 698, 699, 700, 701, 702, 703, 704, 732, 733, 734, 735, 736, 737, 738, 739, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 820, 821, 822, 823, 824, 825, 826, 827, 828, 856, 857, 858, 859, 860, 861, 862, 863, 884, 895, 910, 920, 931, 946, 956, 967, 982]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zvRB2ROqCgc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "716a2bc2-2106-41bc-b2fd-c4dac590cfc1"
      },
      "source": [
        "print('Graos=',Grain)\n",
        "print('Others=',Others)\n",
        "print('Others local',Others_Local)\n",
        "sampled_list = random.sample(Others_Local,Grain)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graos= 254\n",
            "Others= 736\n",
            "Others local [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 911, 912, 913, 914, 915, 916, 917, 918, 919, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 947, 948, 949, 950, 951, 952, 953, 954, 955, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 983, 984, 985, 986, 987, 988, 989]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgjoSbt0Chl-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "32c25d31-ae21-4398-f928-da0581601255"
      },
      "source": [
        "print('Graos=',Grain)\n",
        "print('Others=',Others)\n",
        "print('Others local',Others_Local)\n",
        "print('Grain local',Grain_Local)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graos= 254\n",
            "Others= 736\n",
            "Others local [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 911, 912, 913, 914, 915, 916, 917, 918, 919, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 947, 948, 949, 950, 951, 952, 953, 954, 955, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 983, 984, 985, 986, 987, 988, 989]\n",
            "Grain local [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 36, 37, 38, 39, 40, 41, 42, 43, 72, 73, 74, 75, 76, 108, 109, 110, 111, 112, 113, 114, 144, 145, 146, 147, 148, 149, 170, 171, 172, 173, 206, 207, 208, 209, 210, 211, 212, 213, 242, 243, 244, 245, 246, 247, 248, 249, 268, 269, 270, 271, 272, 273, 274, 275, 276, 304, 305, 306, 307, 308, 309, 310, 340, 341, 342, 343, 344, 345, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 402, 403, 404, 405, 406, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 500, 501, 502, 503, 504, 505, 506, 507, 508, 536, 537, 538, 539, 540, 541, 542, 543, 544, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 588, 589, 590, 591, 592, 593, 594, 624, 625, 626, 627, 628, 629, 630, 631, 632, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 696, 697, 698, 699, 700, 701, 702, 703, 704, 732, 733, 734, 735, 736, 737, 738, 739, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 820, 821, 822, 823, 824, 825, 826, 827, 828, 856, 857, 858, 859, 860, 861, 862, 863, 884, 895, 910, 920, 931, 946, 956, 967, 982]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkM-s9TiCyLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_new=[]\n",
        "y_rede=[]\n",
        "for i in range(Grain):\n",
        "  x_new.append(Fotos.iloc[Grain_Local[i]])\n",
        "  x_new.append(Fotos.iloc[sampled_list[i]])\n",
        "  y_rede.append(y_new[Grain_Local[i]])\n",
        "  y_rede.append(y_new[sampled_list[i]])\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qHR83qhGa0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "089447b9-880a-4e1b-97f6-a1ce9cf01956"
      },
      "source": [
        "np.array(x_new).shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(508, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZp3WDdEFu3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "ad873d7c-2b7f-4d5d-f57a-a4b05521815f"
      },
      "source": [
        "Fotos"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00315</td>\n",
              "      <td>0.00851</td>\n",
              "      <td>0.04608</td>\n",
              "      <td>0.16103</td>\n",
              "      <td>0.24925</td>\n",
              "      <td>0.34163</td>\n",
              "      <td>0.40151</td>\n",
              "      <td>0.42363</td>\n",
              "      <td>0.46514</td>\n",
              "      <td>0.52404</td>\n",
              "      <td>0.54213</td>\n",
              "      <td>0.52907</td>\n",
              "      <td>0.49368</td>\n",
              "      <td>0.47656</td>\n",
              "      <td>0.46535</td>\n",
              "      <td>0.44293</td>\n",
              "      <td>0.43458</td>\n",
              "      <td>0.54960</td>\n",
              "      <td>0.62182</td>\n",
              "      <td>0.65402</td>\n",
              "      <td>0.67005</td>\n",
              "      <td>0.68404</td>\n",
              "      <td>0.70101</td>\n",
              "      <td>0.72188</td>\n",
              "      <td>0.72956</td>\n",
              "      <td>0.72542</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00090</td>\n",
              "      <td>0.00816</td>\n",
              "      <td>0.04123</td>\n",
              "      <td>0.16722</td>\n",
              "      <td>0.28024</td>\n",
              "      <td>0.36929</td>\n",
              "      <td>0.43575</td>\n",
              "      <td>0.48596</td>\n",
              "      <td>0.52757</td>\n",
              "      <td>0.55334</td>\n",
              "      <td>...</td>\n",
              "      <td>0.68837</td>\n",
              "      <td>0.57247</td>\n",
              "      <td>0.50577</td>\n",
              "      <td>0.40034</td>\n",
              "      <td>0.42005</td>\n",
              "      <td>0.49448</td>\n",
              "      <td>0.54021</td>\n",
              "      <td>0.63239</td>\n",
              "      <td>0.66913</td>\n",
              "      <td>0.67656</td>\n",
              "      <td>0.67655</td>\n",
              "      <td>0.68100</td>\n",
              "      <td>0.00130</td>\n",
              "      <td>0.01038</td>\n",
              "      <td>0.01590</td>\n",
              "      <td>0.09138</td>\n",
              "      <td>0.49218</td>\n",
              "      <td>0.66831</td>\n",
              "      <td>0.69867</td>\n",
              "      <td>0.70930</td>\n",
              "      <td>0.70750</td>\n",
              "      <td>0.69464</td>\n",
              "      <td>0.67615</td>\n",
              "      <td>0.65429</td>\n",
              "      <td>0.65067</td>\n",
              "      <td>0.74506</td>\n",
              "      <td>0.72622</td>\n",
              "      <td>0.59420</td>\n",
              "      <td>0.53675</td>\n",
              "      <td>0.40354</td>\n",
              "      <td>0.28763</td>\n",
              "      <td>0.32523</td>\n",
              "      <td>0.40626</td>\n",
              "      <td>0.47360</td>\n",
              "      <td>0.50701</td>\n",
              "      <td>0.63270</td>\n",
              "      <td>0.67356</td>\n",
              "      <td>0.68642</td>\n",
              "      <td>0.68631</td>\n",
              "      <td>0.68769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00309</td>\n",
              "      <td>0.00781</td>\n",
              "      <td>0.08288</td>\n",
              "      <td>0.25110</td>\n",
              "      <td>0.38233</td>\n",
              "      <td>0.51411</td>\n",
              "      <td>0.56924</td>\n",
              "      <td>0.58027</td>\n",
              "      <td>0.60092</td>\n",
              "      <td>0.60788</td>\n",
              "      <td>0.60130</td>\n",
              "      <td>0.59419</td>\n",
              "      <td>0.59183</td>\n",
              "      <td>0.60795</td>\n",
              "      <td>0.64601</td>\n",
              "      <td>0.67848</td>\n",
              "      <td>0.69677</td>\n",
              "      <td>0.70545</td>\n",
              "      <td>0.73198</td>\n",
              "      <td>0.74344</td>\n",
              "      <td>0.74678</td>\n",
              "      <td>0.74795</td>\n",
              "      <td>0.70517</td>\n",
              "      <td>0.63964</td>\n",
              "      <td>0.62419</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00309</td>\n",
              "      <td>0.01360</td>\n",
              "      <td>0.04156</td>\n",
              "      <td>0.19889</td>\n",
              "      <td>0.38310</td>\n",
              "      <td>0.54152</td>\n",
              "      <td>0.59908</td>\n",
              "      <td>0.61056</td>\n",
              "      <td>0.61492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.83302</td>\n",
              "      <td>0.87444</td>\n",
              "      <td>0.87989</td>\n",
              "      <td>0.83853</td>\n",
              "      <td>0.68274</td>\n",
              "      <td>0.52879</td>\n",
              "      <td>0.50917</td>\n",
              "      <td>0.51866</td>\n",
              "      <td>0.49799</td>\n",
              "      <td>0.45938</td>\n",
              "      <td>0.48510</td>\n",
              "      <td>0.52434</td>\n",
              "      <td>0.00784</td>\n",
              "      <td>0.00392</td>\n",
              "      <td>0.00701</td>\n",
              "      <td>0.01031</td>\n",
              "      <td>0.04876</td>\n",
              "      <td>0.33406</td>\n",
              "      <td>0.37898</td>\n",
              "      <td>0.36357</td>\n",
              "      <td>0.34806</td>\n",
              "      <td>0.31723</td>\n",
              "      <td>0.29845</td>\n",
              "      <td>0.30391</td>\n",
              "      <td>0.31748</td>\n",
              "      <td>0.39056</td>\n",
              "      <td>0.48932</td>\n",
              "      <td>0.50467</td>\n",
              "      <td>0.53739</td>\n",
              "      <td>0.54076</td>\n",
              "      <td>0.52627</td>\n",
              "      <td>0.49936</td>\n",
              "      <td>0.48514</td>\n",
              "      <td>0.48982</td>\n",
              "      <td>0.49884</td>\n",
              "      <td>0.50551</td>\n",
              "      <td>0.49065</td>\n",
              "      <td>0.47277</td>\n",
              "      <td>0.49262</td>\n",
              "      <td>0.50909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.45321</td>\n",
              "      <td>0.52257</td>\n",
              "      <td>0.56642</td>\n",
              "      <td>0.56989</td>\n",
              "      <td>0.43670</td>\n",
              "      <td>0.36100</td>\n",
              "      <td>0.37534</td>\n",
              "      <td>0.36939</td>\n",
              "      <td>0.35353</td>\n",
              "      <td>0.35494</td>\n",
              "      <td>0.35494</td>\n",
              "      <td>0.31106</td>\n",
              "      <td>0.24504</td>\n",
              "      <td>0.24274</td>\n",
              "      <td>0.27542</td>\n",
              "      <td>0.26774</td>\n",
              "      <td>0.26129</td>\n",
              "      <td>0.25811</td>\n",
              "      <td>0.25810</td>\n",
              "      <td>0.26644</td>\n",
              "      <td>0.29359</td>\n",
              "      <td>0.30844</td>\n",
              "      <td>0.31510</td>\n",
              "      <td>0.32411</td>\n",
              "      <td>0.33934</td>\n",
              "      <td>0.35733</td>\n",
              "      <td>0.37670</td>\n",
              "      <td>0.39988</td>\n",
              "      <td>0.04326</td>\n",
              "      <td>0.05552</td>\n",
              "      <td>0.07551</td>\n",
              "      <td>0.09699</td>\n",
              "      <td>0.18983</td>\n",
              "      <td>0.32901</td>\n",
              "      <td>0.35796</td>\n",
              "      <td>0.36395</td>\n",
              "      <td>0.36040</td>\n",
              "      <td>0.36140</td>\n",
              "      <td>0.35736</td>\n",
              "      <td>0.32065</td>\n",
              "      <td>...</td>\n",
              "      <td>0.52726</td>\n",
              "      <td>0.54463</td>\n",
              "      <td>0.55229</td>\n",
              "      <td>0.54773</td>\n",
              "      <td>0.53104</td>\n",
              "      <td>0.52842</td>\n",
              "      <td>0.52980</td>\n",
              "      <td>0.52848</td>\n",
              "      <td>0.53234</td>\n",
              "      <td>0.53154</td>\n",
              "      <td>0.52832</td>\n",
              "      <td>0.52296</td>\n",
              "      <td>0.35223</td>\n",
              "      <td>0.39280</td>\n",
              "      <td>0.45083</td>\n",
              "      <td>0.50061</td>\n",
              "      <td>0.54911</td>\n",
              "      <td>0.57563</td>\n",
              "      <td>0.53846</td>\n",
              "      <td>0.44977</td>\n",
              "      <td>0.42229</td>\n",
              "      <td>0.38602</td>\n",
              "      <td>0.33539</td>\n",
              "      <td>0.32225</td>\n",
              "      <td>0.34942</td>\n",
              "      <td>0.40470</td>\n",
              "      <td>0.44376</td>\n",
              "      <td>0.48571</td>\n",
              "      <td>0.53258</td>\n",
              "      <td>0.55060</td>\n",
              "      <td>0.56863</td>\n",
              "      <td>0.55021</td>\n",
              "      <td>0.52429</td>\n",
              "      <td>0.51987</td>\n",
              "      <td>0.51977</td>\n",
              "      <td>0.52343</td>\n",
              "      <td>0.53468</td>\n",
              "      <td>0.54645</td>\n",
              "      <td>0.54919</td>\n",
              "      <td>0.55439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75367</td>\n",
              "      <td>0.64590</td>\n",
              "      <td>0.62497</td>\n",
              "      <td>0.63117</td>\n",
              "      <td>0.60772</td>\n",
              "      <td>0.59781</td>\n",
              "      <td>0.59272</td>\n",
              "      <td>0.58272</td>\n",
              "      <td>0.57275</td>\n",
              "      <td>0.56277</td>\n",
              "      <td>0.55296</td>\n",
              "      <td>0.55087</td>\n",
              "      <td>0.53685</td>\n",
              "      <td>0.50884</td>\n",
              "      <td>0.47949</td>\n",
              "      <td>0.47467</td>\n",
              "      <td>0.46921</td>\n",
              "      <td>0.46706</td>\n",
              "      <td>0.46354</td>\n",
              "      <td>0.42246</td>\n",
              "      <td>0.35577</td>\n",
              "      <td>0.33393</td>\n",
              "      <td>0.32994</td>\n",
              "      <td>0.32207</td>\n",
              "      <td>0.47425</td>\n",
              "      <td>0.66448</td>\n",
              "      <td>0.71360</td>\n",
              "      <td>0.71226</td>\n",
              "      <td>0.76318</td>\n",
              "      <td>0.63869</td>\n",
              "      <td>0.58824</td>\n",
              "      <td>0.56483</td>\n",
              "      <td>0.54475</td>\n",
              "      <td>0.54590</td>\n",
              "      <td>0.54077</td>\n",
              "      <td>0.53631</td>\n",
              "      <td>0.52910</td>\n",
              "      <td>0.52916</td>\n",
              "      <td>0.53897</td>\n",
              "      <td>0.53709</td>\n",
              "      <td>...</td>\n",
              "      <td>0.48987</td>\n",
              "      <td>0.49411</td>\n",
              "      <td>0.49043</td>\n",
              "      <td>0.46473</td>\n",
              "      <td>0.41648</td>\n",
              "      <td>0.38730</td>\n",
              "      <td>0.37650</td>\n",
              "      <td>0.40462</td>\n",
              "      <td>0.42461</td>\n",
              "      <td>0.41316</td>\n",
              "      <td>0.47899</td>\n",
              "      <td>0.61376</td>\n",
              "      <td>0.46481</td>\n",
              "      <td>0.46770</td>\n",
              "      <td>0.47330</td>\n",
              "      <td>0.47886</td>\n",
              "      <td>0.46485</td>\n",
              "      <td>0.45000</td>\n",
              "      <td>0.44123</td>\n",
              "      <td>0.43783</td>\n",
              "      <td>0.43812</td>\n",
              "      <td>0.44221</td>\n",
              "      <td>0.45076</td>\n",
              "      <td>0.45772</td>\n",
              "      <td>0.45711</td>\n",
              "      <td>0.45830</td>\n",
              "      <td>0.47089</td>\n",
              "      <td>0.48372</td>\n",
              "      <td>0.49282</td>\n",
              "      <td>0.50707</td>\n",
              "      <td>0.50889</td>\n",
              "      <td>0.51489</td>\n",
              "      <td>0.50507</td>\n",
              "      <td>0.46091</td>\n",
              "      <td>0.42387</td>\n",
              "      <td>0.42002</td>\n",
              "      <td>0.43764</td>\n",
              "      <td>0.43356</td>\n",
              "      <td>0.48524</td>\n",
              "      <td>0.61876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.94521</td>\n",
              "      <td>0.89736</td>\n",
              "      <td>0.61852</td>\n",
              "      <td>0.51112</td>\n",
              "      <td>0.48211</td>\n",
              "      <td>0.44176</td>\n",
              "      <td>0.47943</td>\n",
              "      <td>0.55011</td>\n",
              "      <td>0.57942</td>\n",
              "      <td>0.58103</td>\n",
              "      <td>0.58870</td>\n",
              "      <td>0.58450</td>\n",
              "      <td>0.56669</td>\n",
              "      <td>0.54924</td>\n",
              "      <td>0.55583</td>\n",
              "      <td>0.67738</td>\n",
              "      <td>0.69853</td>\n",
              "      <td>0.70482</td>\n",
              "      <td>0.71610</td>\n",
              "      <td>0.73135</td>\n",
              "      <td>0.73698</td>\n",
              "      <td>0.74835</td>\n",
              "      <td>0.74909</td>\n",
              "      <td>0.73197</td>\n",
              "      <td>0.71697</td>\n",
              "      <td>0.71570</td>\n",
              "      <td>0.71089</td>\n",
              "      <td>0.70216</td>\n",
              "      <td>0.94706</td>\n",
              "      <td>0.89007</td>\n",
              "      <td>0.64413</td>\n",
              "      <td>0.56434</td>\n",
              "      <td>0.60500</td>\n",
              "      <td>0.65920</td>\n",
              "      <td>0.70258</td>\n",
              "      <td>0.71721</td>\n",
              "      <td>0.69906</td>\n",
              "      <td>0.65997</td>\n",
              "      <td>0.65315</td>\n",
              "      <td>0.65525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.69944</td>\n",
              "      <td>0.68747</td>\n",
              "      <td>0.67009</td>\n",
              "      <td>0.57161</td>\n",
              "      <td>0.51299</td>\n",
              "      <td>0.62049</td>\n",
              "      <td>0.77606</td>\n",
              "      <td>0.95059</td>\n",
              "      <td>0.98506</td>\n",
              "      <td>0.98854</td>\n",
              "      <td>0.98498</td>\n",
              "      <td>0.97282</td>\n",
              "      <td>0.62164</td>\n",
              "      <td>0.63590</td>\n",
              "      <td>0.65889</td>\n",
              "      <td>0.66631</td>\n",
              "      <td>0.67730</td>\n",
              "      <td>0.67407</td>\n",
              "      <td>0.63430</td>\n",
              "      <td>0.56748</td>\n",
              "      <td>0.53108</td>\n",
              "      <td>0.54691</td>\n",
              "      <td>0.59201</td>\n",
              "      <td>0.63914</td>\n",
              "      <td>0.66752</td>\n",
              "      <td>0.67940</td>\n",
              "      <td>0.68352</td>\n",
              "      <td>0.69941</td>\n",
              "      <td>0.70760</td>\n",
              "      <td>0.69960</td>\n",
              "      <td>0.68637</td>\n",
              "      <td>0.61679</td>\n",
              "      <td>0.52409</td>\n",
              "      <td>0.68172</td>\n",
              "      <td>0.92506</td>\n",
              "      <td>0.97829</td>\n",
              "      <td>0.98909</td>\n",
              "      <td>0.98942</td>\n",
              "      <td>0.98108</td>\n",
              "      <td>0.95838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>84.33883</td>\n",
              "      <td>93.31998</td>\n",
              "      <td>103.44733</td>\n",
              "      <td>108.41500</td>\n",
              "      <td>112.13331</td>\n",
              "      <td>119.69690</td>\n",
              "      <td>127.60008</td>\n",
              "      <td>130.90379</td>\n",
              "      <td>126.89029</td>\n",
              "      <td>118.48801</td>\n",
              "      <td>126.34141</td>\n",
              "      <td>126.96152</td>\n",
              "      <td>123.98750</td>\n",
              "      <td>118.19897</td>\n",
              "      <td>108.84884</td>\n",
              "      <td>96.85558</td>\n",
              "      <td>92.20214</td>\n",
              "      <td>89.77406</td>\n",
              "      <td>84.23468</td>\n",
              "      <td>80.21286</td>\n",
              "      <td>75.36819</td>\n",
              "      <td>67.87563</td>\n",
              "      <td>62.45944</td>\n",
              "      <td>64.41500</td>\n",
              "      <td>62.72327</td>\n",
              "      <td>60.94029</td>\n",
              "      <td>59.12835</td>\n",
              "      <td>56.51498</td>\n",
              "      <td>83.05574</td>\n",
              "      <td>89.28664</td>\n",
              "      <td>100.81055</td>\n",
              "      <td>107.22555</td>\n",
              "      <td>111.19422</td>\n",
              "      <td>120.95100</td>\n",
              "      <td>129.88733</td>\n",
              "      <td>130.47650</td>\n",
              "      <td>120.22813</td>\n",
              "      <td>114.69511</td>\n",
              "      <td>127.27454</td>\n",
              "      <td>129.73557</td>\n",
              "      <td>...</td>\n",
              "      <td>1.27197</td>\n",
              "      <td>1.10851</td>\n",
              "      <td>0.93196</td>\n",
              "      <td>0.68677</td>\n",
              "      <td>0.46082</td>\n",
              "      <td>0.73854</td>\n",
              "      <td>0.73854</td>\n",
              "      <td>0.73854</td>\n",
              "      <td>0.59968</td>\n",
              "      <td>0.34418</td>\n",
              "      <td>0.34418</td>\n",
              "      <td>0.34418</td>\n",
              "      <td>7.18766</td>\n",
              "      <td>7.36263</td>\n",
              "      <td>7.64868</td>\n",
              "      <td>7.34596</td>\n",
              "      <td>7.39873</td>\n",
              "      <td>7.52093</td>\n",
              "      <td>7.68201</td>\n",
              "      <td>8.29300</td>\n",
              "      <td>8.17913</td>\n",
              "      <td>7.74033</td>\n",
              "      <td>7.85975</td>\n",
              "      <td>7.73755</td>\n",
              "      <td>7.78199</td>\n",
              "      <td>8.37909</td>\n",
              "      <td>9.67368</td>\n",
              "      <td>12.75441</td>\n",
              "      <td>9.18905</td>\n",
              "      <td>8.16465</td>\n",
              "      <td>7.41916</td>\n",
              "      <td>7.29776</td>\n",
              "      <td>7.70423</td>\n",
              "      <td>8.11526</td>\n",
              "      <td>7.92918</td>\n",
              "      <td>7.87364</td>\n",
              "      <td>7.87919</td>\n",
              "      <td>7.52371</td>\n",
              "      <td>7.14600</td>\n",
              "      <td>7.38484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>104.73698</td>\n",
              "      <td>108.48595</td>\n",
              "      <td>108.84282</td>\n",
              "      <td>109.37420</td>\n",
              "      <td>93.64349</td>\n",
              "      <td>79.20728</td>\n",
              "      <td>42.58791</td>\n",
              "      <td>36.74109</td>\n",
              "      <td>38.59599</td>\n",
              "      <td>36.43092</td>\n",
              "      <td>35.09443</td>\n",
              "      <td>36.54703</td>\n",
              "      <td>40.69532</td>\n",
              "      <td>42.58339</td>\n",
              "      <td>42.79250</td>\n",
              "      <td>42.07663</td>\n",
              "      <td>43.08405</td>\n",
              "      <td>43.43935</td>\n",
              "      <td>41.89973</td>\n",
              "      <td>41.28051</td>\n",
              "      <td>43.74358</td>\n",
              "      <td>43.05623</td>\n",
              "      <td>43.89751</td>\n",
              "      <td>46.14607</td>\n",
              "      <td>47.10162</td>\n",
              "      <td>46.39990</td>\n",
              "      <td>43.81701</td>\n",
              "      <td>29.31175</td>\n",
              "      <td>92.94746</td>\n",
              "      <td>97.39994</td>\n",
              "      <td>97.17322</td>\n",
              "      <td>96.06416</td>\n",
              "      <td>89.57974</td>\n",
              "      <td>70.81004</td>\n",
              "      <td>36.06353</td>\n",
              "      <td>38.26520</td>\n",
              "      <td>39.27092</td>\n",
              "      <td>37.12646</td>\n",
              "      <td>39.01833</td>\n",
              "      <td>43.09563</td>\n",
              "      <td>...</td>\n",
              "      <td>48.82171</td>\n",
              "      <td>40.17374</td>\n",
              "      <td>35.79261</td>\n",
              "      <td>34.48748</td>\n",
              "      <td>31.60311</td>\n",
              "      <td>28.78050</td>\n",
              "      <td>25.19526</td>\n",
              "      <td>18.31288</td>\n",
              "      <td>9.99981</td>\n",
              "      <td>5.29681</td>\n",
              "      <td>1.19395</td>\n",
              "      <td>0.03982</td>\n",
              "      <td>71.93044</td>\n",
              "      <td>74.55727</td>\n",
              "      <td>74.04531</td>\n",
              "      <td>73.32433</td>\n",
              "      <td>72.92789</td>\n",
              "      <td>75.42004</td>\n",
              "      <td>72.73616</td>\n",
              "      <td>68.83771</td>\n",
              "      <td>65.64970</td>\n",
              "      <td>64.07162</td>\n",
              "      <td>63.85171</td>\n",
              "      <td>64.07370</td>\n",
              "      <td>64.69039</td>\n",
              "      <td>67.68124</td>\n",
              "      <td>77.33430</td>\n",
              "      <td>77.30132</td>\n",
              "      <td>66.68658</td>\n",
              "      <td>48.47966</td>\n",
              "      <td>31.81008</td>\n",
              "      <td>26.50224</td>\n",
              "      <td>25.00339</td>\n",
              "      <td>24.54804</td>\n",
              "      <td>22.74497</td>\n",
              "      <td>19.09086</td>\n",
              "      <td>14.28277</td>\n",
              "      <td>7.58692</td>\n",
              "      <td>2.05457</td>\n",
              "      <td>0.38150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>75.70248</td>\n",
              "      <td>76.11571</td>\n",
              "      <td>76.23968</td>\n",
              "      <td>75.57852</td>\n",
              "      <td>77.09092</td>\n",
              "      <td>79.73554</td>\n",
              "      <td>81.92563</td>\n",
              "      <td>83.74380</td>\n",
              "      <td>85.03307</td>\n",
              "      <td>85.42976</td>\n",
              "      <td>84.70248</td>\n",
              "      <td>83.08265</td>\n",
              "      <td>83.28101</td>\n",
              "      <td>82.20662</td>\n",
              "      <td>81.11571</td>\n",
              "      <td>80.66943</td>\n",
              "      <td>80.48761</td>\n",
              "      <td>78.95042</td>\n",
              "      <td>78.64463</td>\n",
              "      <td>76.69422</td>\n",
              "      <td>74.05785</td>\n",
              "      <td>74.55373</td>\n",
              "      <td>79.70248</td>\n",
              "      <td>89.48761</td>\n",
              "      <td>92.97521</td>\n",
              "      <td>92.54546</td>\n",
              "      <td>92.64464</td>\n",
              "      <td>101.02480</td>\n",
              "      <td>76.71075</td>\n",
              "      <td>81.68595</td>\n",
              "      <td>84.04133</td>\n",
              "      <td>85.95868</td>\n",
              "      <td>85.96695</td>\n",
              "      <td>80.38017</td>\n",
              "      <td>79.95868</td>\n",
              "      <td>81.94215</td>\n",
              "      <td>83.89257</td>\n",
              "      <td>84.12398</td>\n",
              "      <td>82.86777</td>\n",
              "      <td>80.95042</td>\n",
              "      <td>...</td>\n",
              "      <td>50.32232</td>\n",
              "      <td>56.40496</td>\n",
              "      <td>74.05786</td>\n",
              "      <td>89.42976</td>\n",
              "      <td>101.38843</td>\n",
              "      <td>112.18182</td>\n",
              "      <td>117.19008</td>\n",
              "      <td>113.55373</td>\n",
              "      <td>104.04960</td>\n",
              "      <td>104.89256</td>\n",
              "      <td>105.57026</td>\n",
              "      <td>106.42976</td>\n",
              "      <td>93.77686</td>\n",
              "      <td>99.57851</td>\n",
              "      <td>108.53720</td>\n",
              "      <td>120.25620</td>\n",
              "      <td>127.38017</td>\n",
              "      <td>134.15704</td>\n",
              "      <td>126.38844</td>\n",
              "      <td>117.23140</td>\n",
              "      <td>107.13223</td>\n",
              "      <td>94.63637</td>\n",
              "      <td>76.44628</td>\n",
              "      <td>79.47108</td>\n",
              "      <td>86.98347</td>\n",
              "      <td>89.67769</td>\n",
              "      <td>75.04959</td>\n",
              "      <td>65.11571</td>\n",
              "      <td>76.28926</td>\n",
              "      <td>86.43803</td>\n",
              "      <td>96.22314</td>\n",
              "      <td>98.80165</td>\n",
              "      <td>105.86777</td>\n",
              "      <td>115.63637</td>\n",
              "      <td>122.77687</td>\n",
              "      <td>124.29753</td>\n",
              "      <td>112.47935</td>\n",
              "      <td>106.21488</td>\n",
              "      <td>107.38844</td>\n",
              "      <td>106.00001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>56.64465</td>\n",
              "      <td>56.24413</td>\n",
              "      <td>54.42472</td>\n",
              "      <td>53.71609</td>\n",
              "      <td>52.32697</td>\n",
              "      <td>53.16669</td>\n",
              "      <td>53.79039</td>\n",
              "      <td>46.68970</td>\n",
              "      <td>17.08013</td>\n",
              "      <td>16.50871</td>\n",
              "      <td>23.22551</td>\n",
              "      <td>31.03260</td>\n",
              "      <td>47.71372</td>\n",
              "      <td>58.25345</td>\n",
              "      <td>59.95493</td>\n",
              "      <td>59.04939</td>\n",
              "      <td>56.77308</td>\n",
              "      <td>52.00291</td>\n",
              "      <td>53.62450</td>\n",
              "      <td>55.88072</td>\n",
              "      <td>55.28537</td>\n",
              "      <td>53.28445</td>\n",
              "      <td>51.44337</td>\n",
              "      <td>52.30184</td>\n",
              "      <td>55.55235</td>\n",
              "      <td>53.35468</td>\n",
              "      <td>47.00618</td>\n",
              "      <td>37.47911</td>\n",
              "      <td>53.69627</td>\n",
              "      <td>54.42475</td>\n",
              "      <td>57.33358</td>\n",
              "      <td>55.24602</td>\n",
              "      <td>54.19571</td>\n",
              "      <td>53.65913</td>\n",
              "      <td>54.56948</td>\n",
              "      <td>48.65750</td>\n",
              "      <td>21.57943</td>\n",
              "      <td>28.26186</td>\n",
              "      <td>38.84935</td>\n",
              "      <td>42.26283</td>\n",
              "      <td>...</td>\n",
              "      <td>54.14144</td>\n",
              "      <td>92.98824</td>\n",
              "      <td>91.59424</td>\n",
              "      <td>80.00546</td>\n",
              "      <td>46.59100</td>\n",
              "      <td>24.38414</td>\n",
              "      <td>23.20038</td>\n",
              "      <td>21.31728</td>\n",
              "      <td>16.15013</td>\n",
              "      <td>13.08959</td>\n",
              "      <td>4.55772</td>\n",
              "      <td>0.69973</td>\n",
              "      <td>98.26086</td>\n",
              "      <td>103.31665</td>\n",
              "      <td>103.88441</td>\n",
              "      <td>106.66310</td>\n",
              "      <td>114.97292</td>\n",
              "      <td>113.56293</td>\n",
              "      <td>111.95552</td>\n",
              "      <td>102.36475</td>\n",
              "      <td>99.74646</td>\n",
              "      <td>102.59967</td>\n",
              "      <td>101.44373</td>\n",
              "      <td>45.99594</td>\n",
              "      <td>6.73845</td>\n",
              "      <td>13.60528</td>\n",
              "      <td>18.16961</td>\n",
              "      <td>40.74703</td>\n",
              "      <td>90.45869</td>\n",
              "      <td>95.27261</td>\n",
              "      <td>94.23108</td>\n",
              "      <td>94.50986</td>\n",
              "      <td>92.19203</td>\n",
              "      <td>61.69930</td>\n",
              "      <td>27.43373</td>\n",
              "      <td>21.54411</td>\n",
              "      <td>17.51231</td>\n",
              "      <td>11.69516</td>\n",
              "      <td>3.15316</td>\n",
              "      <td>0.40107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>111.50165</td>\n",
              "      <td>117.65936</td>\n",
              "      <td>105.70792</td>\n",
              "      <td>96.50142</td>\n",
              "      <td>93.97151</td>\n",
              "      <td>96.16643</td>\n",
              "      <td>97.13624</td>\n",
              "      <td>101.40429</td>\n",
              "      <td>108.54787</td>\n",
              "      <td>115.69933</td>\n",
              "      <td>130.90604</td>\n",
              "      <td>150.12636</td>\n",
              "      <td>128.60452</td>\n",
              "      <td>39.56743</td>\n",
              "      <td>43.49378</td>\n",
              "      <td>43.60303</td>\n",
              "      <td>45.61090</td>\n",
              "      <td>46.42926</td>\n",
              "      <td>46.44329</td>\n",
              "      <td>46.05452</td>\n",
              "      <td>42.04240</td>\n",
              "      <td>70.33170</td>\n",
              "      <td>94.53501</td>\n",
              "      <td>100.75374</td>\n",
              "      <td>100.08746</td>\n",
              "      <td>100.16441</td>\n",
              "      <td>95.87692</td>\n",
              "      <td>91.88138</td>\n",
              "      <td>105.59963</td>\n",
              "      <td>104.88435</td>\n",
              "      <td>102.00934</td>\n",
              "      <td>100.97969</td>\n",
              "      <td>100.34370</td>\n",
              "      <td>98.18790</td>\n",
              "      <td>98.55000</td>\n",
              "      <td>102.79296</td>\n",
              "      <td>103.48549</td>\n",
              "      <td>115.63513</td>\n",
              "      <td>126.47156</td>\n",
              "      <td>120.13804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.52811</td>\n",
              "      <td>0.50887</td>\n",
              "      <td>0.77713</td>\n",
              "      <td>0.65724</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.03571</td>\n",
              "      <td>0.40323</td>\n",
              "      <td>0.08184</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.09672</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.73238</td>\n",
              "      <td>0.61558</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>990 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0          1          2  ...        781        782        783\n",
              "0      0.00000    0.00000    0.00315  ...    0.68642    0.68631    0.68769\n",
              "1      0.00000    0.00000    0.00000  ...    0.47277    0.49262    0.50909\n",
              "2      0.45321    0.52257    0.56642  ...    0.54645    0.54919    0.55439\n",
              "3      0.75367    0.64590    0.62497  ...    0.43356    0.48524    0.61876\n",
              "4      0.94521    0.89736    0.61852  ...    0.98942    0.98108    0.95838\n",
              "..         ...        ...        ...  ...        ...        ...        ...\n",
              "985   84.33883   93.31998  103.44733  ...    7.52371    7.14600    7.38484\n",
              "986  104.73698  108.48595  108.84282  ...    7.58692    2.05457    0.38150\n",
              "987   75.70248   76.11571   76.23968  ...  106.21488  107.38844  106.00001\n",
              "988   56.64465   56.24413   54.42472  ...   11.69516    3.15316    0.40107\n",
              "989  111.50165  117.65936  105.70792  ...    0.00000    0.00000    0.00000\n",
              "\n",
              "[990 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb33el9Y9AXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_valor=np.copy(y_new)\n",
        "y_valor=np.copy(y_rede)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSs3qh0_uO94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define data train and data test\n",
        "#images28=Fotos\n",
        "images28=x_new #Fotos\n",
        "W_train, W_test, yw_train, yw_test = train_test_split(np.array(images28), np.array(y_valor), \n",
        "                                                    test_size=0.30, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwHcagDGuow0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images=W_train #imagens utilizadas para o treino\n",
        "train_labels=yw_train # resposta da rede\n",
        "test_images=W_test\n",
        "test_labels=yw_test"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWSEclzGurul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w88iZqfVusm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0MAtbvxuwQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "27294662-0a8e-4266-aa38-162a1fda26bd"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"flatten_input:0\", shape=(None, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, 784).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"flatten_input:0\", shape=(None, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, 784).\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 5.0455 - accuracy: 0.4282\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.2265 - accuracy: 0.5155\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.8255 - accuracy: 0.5070\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.9728 - accuracy: 0.5549\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.9254 - accuracy: 0.5718\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.8301 - accuracy: 0.6423\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.2819 - accuracy: 0.6056\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.9448 - accuracy: 0.6310\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.6334 - accuracy: 0.7070\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.3986 - accuracy: 0.5127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f84cf9b3b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0BTZBnKuy5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "827a5665-b552-4743-d805-17456209a398"
      },
      "source": [
        "#ANN das imagens recortadas por nós\n",
        "x=images28 \n",
        "logits = model(x, training=False)\n",
        "prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "#print(prediction)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-459aaf4775e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ANN das imagens recortadas por nós\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    717\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5575\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5576\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5577\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5578\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5579\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [784,1], In[1]: [784,128] [Op:MatMul]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU-3mMSMu6a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {'y_Actual': y_valor,\n",
        "        'y_Predicted': prediction\n",
        "        }\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
        "#print (df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u48SDVPavRwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
        "print (confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}